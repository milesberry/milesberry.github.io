---
layout: post
title: "How important is the human touch in the age of AI learning?"
date: "2024-05-03 6:00:00 +0000"
author: Miles Berry
permalink: /2024/05/ai-he/
comments: true
image:
        feature: 240503.jpg
---

Technologies in education have historically opened up access, making what was once the preserve of a few available to many. While elitist, Plato's academy replaced private tutoring, making education available to any free males who had the capacity and desire to study. The written word allowed the literate to learn from others across time and space. As universities supplanted monasteries, European higher education expanded beyond religious orders and theology. The printing press massively increased access to the written word, with libraries becoming as vital as classrooms or lecture rooms. 

More recently, universal schooling and a vast expansion of university education have further democratised learning. The internet, the World Wide Web and social media have amplified access to content and enabled anyone to create and share it, albeit with variable quality control.

I propose we view generative AI similarly - as a technology making previously exclusive interactions available to all. 

How does university education work today? Largely through lectures presenting content, seminars encouraging discussion between academics and students, access to libraries of resources, assignments, and feedback. Of course, practicals and placements are also central for many disciplines. But I'd wager your experience isn't radically different from mine.

At least a couple of universities employ another teaching approach centred around weekly tutorials or supervisions. In these sessions, one or two students meet with an academic tutor to discuss work prepared in advance. The tutor then provides detailed feedback and critique on the student's work. Interestingly, these universities typically enrol highly confident, knowledgeable, and independent students, who probably need this level of support and engagement far less than students elsewhere.

The large language models are much like the best tutors - well read, articulate, patient, adaptable and generally helpful, even if the AIs are perhaps more willing than most academics to make things up when they’re not sure. With ChatGPT and other large language models, we now have an opportunity to bring this tutorial based approach to education for all students. Students could share each week’s content with the AI, which could then generate problems or essay titles, suggest readings and approaches. The language models can provide support when needed, offer encouragement, and provide in-depth critiques, highlighting gaps, mistakes and misconceptions in the student's work.

For me, this has to be one of the best ways students could leverage language models to get a personalised, if not quite personal, higher education experience. Yes, it wouldn't have the human touch of individual tutorials, but it's inexpensive and available now to any student willing to use it. Shouldn't we be encouraging them, expecting them, even requiring them to do so?

Beyond tutorial support, allow me to share some other insights on generative AI in universities to spur our dinner conversations.

The language models have far more patience than most tutors, and as 'transformers' are very skilled at explaining things in different ways, making the content of our courses far more accessible than they would otherwise be. Many students, not just those whose first language isn’t English or with dyslexia, will struggle to read, understand and critique journal articles, so why not work with an AI generated summary and critique? 

Many of us aim to ensure our graduates are well-positioned for employment. It seems the professions will increasingly make use of AI, so university should prepare students for these new opportunities and responsibilities. I believe this means integrating disciplinary and vocational AI applications into our programmes, and aligning at least some assessments to this new reality if they are to remain authentic and valid.

Is it enough for graduates to become skilled at using current AI tools? Probably not for the long term - some understanding of how they work seems crucial. Perhaps 'Foundations of AI' courses could join interdisciplinary modules like academic writing, critical thinking, and quantitative approaches for all students.  I think students (and colleagues) should grasp how language models are trained and how they produce responses based on the tokens seen so far, and how this can lead to bias and hallucination.

I'm less persuaded that we should teach prompt engineering or the like - since these tools use natural language, the interface is quite intuitive. Engaging in discussion, asking for reasons or step-by-step workings is pretty intuitive. My conversations with ChatGPT often resemble tutorials with students. What does matter for prompting well is domain knowledge - recognizing its importance for getting good responses and judging their quality. This comforts me about the enduring role of a university education.

There's an awareness that model outputs need scrutiny, and I hope students extend this criticality to other sources and even lecture content. Because the models use language so well, it's easy to forget we're not communicating with a person - there is no lived experience, actual understanding or reasoning here, although these can be faked convincingly. How can we assess if a student truly understands something, other than by their ability to explain or apply it using language? It turns out, the machines are surprisingly good at just that.

Which brings me, inevitably, to assessment. If we want to ensure students can recall and apply knowledge to novel problems, then proctored, handwritten exams will endure for now.

For any coursework not done under controlled conditions, it's hard to verify the work is the candidate's own. This problem predates AI - witness contract cheating and plagiarism - but language models make cheating much more accessible and lower-risk. AI plagiarism detectors aren't reliable, and students seem willing to experiment in order to beat these, like my daughter asking ChatGPT to write her history homework to seem dyslexic and avoid scrutiny.

There's a huge risk of false positives here, offending due process principles. That said, the default of anonymous submissions might be ripe for reconsideration - putting names on papers might give students pause before submitting AI work, and help instructors judge whether an in-person discussion is warranted, without accusation.

I think we should clarify what's acceptable and what's not. If students might reasonably ask a peer or tutor to explain a concept, suggest readings, or give feedback, then why not an AI? Just as they shouldn't ask others to do the work for them, they shouldn't use AI that way either. I'd like to see acknowledgments of help - from tutors, librarians, friends, and AIs - included in all academic work.

Academic dishonesty, like fraud, requires motive, means, and rationalisation. For many students, the temptation for an easy option persists strongly, and the means are more available than ever. But we can address the rationalisation by persuading students that assignments aren't merely assessments, but where real learning happens - through the act of writing or creating, they develop their understanding of what they came to study.

Turing proposed a test - that if an expert couldn't reliably distinguish machine-generated text from human-generated, it would be reasonable to call the machine intelligent in some sense. Ironically, to use ChatGPT, I must now pass a Turing test of sorts. I think there is more to being human than ticking boxes or identifying traffic lights.

Much of what made me who I am stems from my time as a student - not just the content I studied, but the experience: grappling with challenging problems, studying hard (if not hard enough) for exams, reading broadly, forging friendships, broadening my interests and horizons. As invaluable as ChatGPT support might have been, it was the human aspects of university that truly shaped me.

I was the first in my family to attend university, and that experience transformed my trajectory in life. Let's ensure generative AI enhances rather than diminishes that formative journey for today's students. It can empower them to reach their potential through personalised support, feedback and challenge. But we must be thoughtful to preserve what makes university special - an initiation into academic life, a rite of passage into adulthood, and an expansion of one's self and world.


*Remarks at a roundtable higher education dinner on 2 May*